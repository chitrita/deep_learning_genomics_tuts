{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling theano functions\n",
      "Starting training...\n",
      "Epoch: 1, train_loss=2.289660, valid_loss=2.239347, valid_accuracy=0.174100, time=0.152448s\n",
      "Epoch: 2, train_loss=2.195213, valid_loss=2.157211, valid_accuracy=0.240400, time=0.143531s\n",
      "Epoch: 3, train_loss=2.120240, valid_loss=2.088282, valid_accuracy=0.295600, time=0.141754s\n",
      "Epoch: 4, train_loss=2.054930, valid_loss=2.026074, valid_accuracy=0.357000, time=0.146649s\n",
      "Epoch: 5, train_loss=1.994707, valid_loss=1.967577, valid_accuracy=0.418100, time=0.170406s\n",
      "Epoch: 6, train_loss=1.937376, valid_loss=1.911292, valid_accuracy=0.473400, time=0.148437s\n",
      "Epoch: 7, train_loss=1.881857, valid_loss=1.856489, valid_accuracy=0.520700, time=0.144898s\n",
      "Epoch: 8, train_loss=1.827674, valid_loss=1.802861, valid_accuracy=0.560600, time=0.142810s\n",
      "Epoch: 9, train_loss=1.774583, valid_loss=1.750262, valid_accuracy=0.592000, time=0.143788s\n",
      "Epoch: 10, train_loss=1.722589, valid_loss=1.698759, valid_accuracy=0.618800, time=0.146223s\n",
      "Epoch: 11, train_loss=1.671765, valid_loss=1.648435, valid_accuracy=0.640600, time=0.146815s\n",
      "Epoch: 12, train_loss=1.622162, valid_loss=1.599392, valid_accuracy=0.659800, time=0.148172s\n",
      "Epoch: 13, train_loss=1.573852, valid_loss=1.551628, valid_accuracy=0.676800, time=0.145791s\n",
      "Epoch: 14, train_loss=1.526943, valid_loss=1.505331, valid_accuracy=0.690900, time=0.147303s\n",
      "Epoch: 15, train_loss=1.481534, valid_loss=1.460608, valid_accuracy=0.705000, time=0.150484s\n",
      "Epoch: 16, train_loss=1.437746, valid_loss=1.417513, valid_accuracy=0.715000, time=0.146781s\n",
      "Epoch: 17, train_loss=1.395663, valid_loss=1.376123, valid_accuracy=0.723700, time=0.148225s\n",
      "Epoch: 18, train_loss=1.355317, valid_loss=1.336458, valid_accuracy=0.732200, time=0.148784s\n",
      "Epoch: 19, train_loss=1.316739, valid_loss=1.298541, valid_accuracy=0.742200, time=0.146991s\n",
      "Epoch: 20, train_loss=1.279933, valid_loss=1.262342, valid_accuracy=0.749900, time=0.152070s\n",
      "Epoch: 21, train_loss=1.244862, valid_loss=1.227836, valid_accuracy=0.756800, time=0.148282s\n",
      "Epoch: 22, train_loss=1.211487, valid_loss=1.194998, valid_accuracy=0.764800, time=0.152546s\n",
      "Epoch: 23, train_loss=1.179758, valid_loss=1.163781, valid_accuracy=0.772100, time=0.178886s\n",
      "Epoch: 24, train_loss=1.149614, valid_loss=1.134132, valid_accuracy=0.778400, time=0.175592s\n",
      "Epoch: 25, train_loss=1.120997, valid_loss=1.105971, valid_accuracy=0.783600, time=0.163702s\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, absolute_import\n",
    "from __future__ import print_function, unicode_literals\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "import sklearn.cross_validation\n",
    "import sklearn.metrics\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "\n",
    "# ############################### prepare data ###############################\n",
    "\n",
    "mnist = sklearn.datasets.fetch_mldata('MNIST original')\n",
    "\n",
    "X = mnist['data'].astype(np.float32) / 255.0\n",
    "y = mnist['target'].astype(\"int32\")\n",
    "X_train, X_valid, y_train, y_valid = sklearn.cross_validation.train_test_split(\n",
    "    X, y, random_state=42, train_size=50000, test_size=10000)\n",
    "X_train = X_train.reshape(-1, 1, 28, 28)\n",
    "X_valid = X_valid.reshape(-1, 1, 28, 28)\n",
    "\n",
    "l_in = lasagne.layers.InputLayer(\n",
    "    shape=(None, 1, 28, 28),\n",
    ")\n",
    "\n",
    "l_hidden1 = lasagne.layers.DenseLayer(\n",
    "    l_in,\n",
    "    num_units=64,\n",
    "    nonlinearity=lasagne.nonlinearities.rectify,\n",
    "    W=lasagne.init.GlorotUniform(),\n",
    ")\n",
    "\n",
    "l_out = lasagne.layers.DenseLayer(\n",
    "    l_hidden1,\n",
    "    num_units=10,\n",
    "    nonlinearity=lasagne.nonlinearities.softmax,\n",
    "    W=lasagne.init.GlorotUniform(),\n",
    ")\n",
    "\n",
    "# ############################### network loss ###############################\n",
    "\n",
    "# int32 vector\n",
    "target_vector = T.ivector('y')\n",
    "\n",
    "\n",
    "def loss_fn(output):\n",
    "    return T.mean(lasagne.objectives.categorical_crossentropy(output,\n",
    "                                                              target_vector))\n",
    "\n",
    "output = lasagne.layers.get_output(l_out)\n",
    "loss = loss_fn(output)\n",
    "\n",
    "# ######################## compiling theano functions ########################\n",
    "\n",
    "print(\"Compiling theano functions\")\n",
    "\n",
    "# - takes out all weight tensors from the network, in order to compute\n",
    "#   how the weights should be updated\n",
    "all_params = lasagne.layers.get_all_params(l_out)\n",
    "\n",
    "# - calculate how the parameters should be updated\n",
    "# - theano keeps a graph of operations, so that gradients w.r.t.\n",
    "#   the loss can be calculated\n",
    "updates = lasagne.updates.sgd(\n",
    "    loss_or_grads=loss,\n",
    "    params=all_params,\n",
    "    learning_rate=0.001)\n",
    "\n",
    "# - create a function that also updates the weights\n",
    "# - this function takes in 2 arguments: the input batch of images and a\n",
    "#   target vector (the y's) and returns a list with a single scalar\n",
    "#   element (the loss)\n",
    "train_fn = theano.function(inputs=[l_in.input_var, target_vector],\n",
    "                           outputs=[loss],\n",
    "                           updates=updates)\n",
    "\n",
    "# - same interface as previous the previous function, but now the\n",
    "#   output is a list where the first element is the loss, and the\n",
    "#   second element is the actual predicted probabilities for the\n",
    "#   input data\n",
    "valid_fn = theano.function(inputs=[l_in.input_var, target_vector],\n",
    "                           outputs=[loss, output])\n",
    "\n",
    "# ################################# training #################################\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "num_epochs = 25\n",
    "batch_size = 600\n",
    "for epoch_num in range(num_epochs):\n",
    "    num_batches_train = int(np.ceil(len(X_train) / batch_size))\n",
    "\n",
    "    train_losses = []\n",
    "    for batch_num in range(num_batches_train):\n",
    "        X_batch = X_train[batch_size * batch_num:batch_size * (batch_num + 1)]\n",
    "        y_batch = y_train[batch_size * batch_num:batch_size * (batch_num + 1)]\n",
    "\n",
    "        loss, = train_fn(X_batch, y_batch)\n",
    "        train_losses.append(loss)\n",
    "    # aggregate training losses for each minibatch into scalar\n",
    "    train_loss = np.mean(train_losses)\n",
    "\n",
    "    # calculate validation loss\n",
    "    num_batches_valid = int(np.ceil(len(X_valid) / batch_size))\n",
    "    valid_losses = []\n",
    "    list_of_probabilities_batch = []\n",
    "    for batch_num in range(num_batches_valid):\n",
    "        X_batch = X_valid[batch_size * batch_num:batch_size * (batch_num + 1)]\n",
    "        y_batch = y_valid[batch_size * batch_num:batch_size * (batch_num + 1)]\n",
    "\n",
    "        loss, probabilities_batch = valid_fn(X_batch, y_batch)\n",
    "        valid_losses.append(loss)\n",
    "        list_of_probabilities_batch.append(probabilities_batch)\n",
    "    valid_loss = np.mean(valid_losses)\n",
    "    # concatenate probabilities for each batch into a matrix\n",
    "    probabilities = np.concatenate(list_of_probabilities_batch)\n",
    "    # calculate classes from the probabilities\n",
    "    predicted_classes = np.argmax(probabilities, axis=1)\n",
    "    # calculate accuracy for this epoch\n",
    "    accuracy = sklearn.metrics.accuracy_score(y_valid, predicted_classes)\n",
    "\n",
    "    print(\"Epoch: %d, train_loss=%f, valid_loss=%f, valid_accuracy=%f\"\n",
    "          % (epoch_num + 1, train_loss, valid_loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualizing things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_images(img_original, saliency):\n",
    "    saliency = saliency[0]\n",
    "    # convert saliency from BGR to RGB, and from c01 to 01c\n",
    "    saliency = saliency[::-1].transpose(1, 2, 0)\n",
    "    # plot the original image and the three saliency map variants\n",
    "    plt.figure(figsize=(10, 10), facecolor='w')\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.title('input')\n",
    "    plt.imshow(img_original)\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.title('abs. saliency')\n",
    "    plt.imshow(np.abs(saliency).max(axis=-1), cmap='gray')\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.title('pos. saliency')\n",
    "    plt.imshow((np.maximum(0, saliency) / saliency.max()))\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.title('neg. saliency')\n",
    "    plt.imshow((np.maximum(0, -saliency) / -saliency.min()))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ModifiedBackprop(object):\n",
    "\n",
    "    def __init__(self, nonlinearity):\n",
    "        self.nonlinearity = nonlinearity\n",
    "        self.ops = {}  # memoizes an OpFromGraph instance per tensor type\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # OpFromGraph is oblique to Theano optimizations, so we need to move\n",
    "        # things to GPU ourselves if needed.\n",
    "        if theano.sandbox.cuda.cuda_enabled:\n",
    "            maybe_to_gpu = theano.sandbox.cuda.as_cuda_ndarray_variable\n",
    "        else:\n",
    "            maybe_to_gpu = lambda x: x\n",
    "        # We move the input to GPU if needed.\n",
    "        x = maybe_to_gpu(x)\n",
    "        # We note the tensor type of the input variable to the nonlinearity\n",
    "        # (mainly dimensionality and dtype); we need to create a fitting Op.\n",
    "        tensor_type = x.type\n",
    "        # If we did not create a suitable Op yet, this is the time to do so.\n",
    "        if tensor_type not in self.ops:\n",
    "            # For the graph, we create an input variable of the correct type:\n",
    "            inp = tensor_type()\n",
    "            # We pass it through the nonlinearity (and move to GPU if needed).\n",
    "            outp = maybe_to_gpu(self.nonlinearity(inp))\n",
    "            # Then we fix the forward expression...\n",
    "            op = theano.OpFromGraph([inp], [outp])\n",
    "            # ...and replace the gradient with our own (defined in a subclass).\n",
    "            op.grad = self.grad\n",
    "            # Finally, we memoize the new Op\n",
    "            self.ops[tensor_type] = op\n",
    "        # And apply the memoized Op to the input we got.\n",
    "        return self.ops[tensor_type](x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GuidedBackprop(ModifiedBackprop):\n",
    "    def grad(self, inputs, out_grads):\n",
    "        (inp,) = inputs\n",
    "        (grd,) = out_grads\n",
    "        dtype = inp.dtype\n",
    "        return (grd * (inp > 0).astype(dtype) * (grd > 0).astype(dtype),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "relu = lasagne.nonlinearities.rectify\n",
    "relu_layers = [layer for layer in lasagne.layers.get_all_layers(net['prob'])\n",
    "               if getattr(layer, 'nonlinearity', None) is relu]\n",
    "modded_relu = GuidedBackprop(relu)  # important: only instantiate this once!\n",
    "for layer in relu_layers:\n",
    "    layer.nonlinearity = modded_relu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
