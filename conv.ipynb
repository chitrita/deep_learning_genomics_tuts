{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, absolute_import\n",
    "from __future__ import print_function, unicode_literals\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "import sklearn.cross_validation\n",
    "import sklearn.metrics\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "\n",
    "# ############################### prepare data ###############################\n",
    "\n",
    "mnist = sklearn.datasets.fetch_mldata('MNIST original')\n",
    "\n",
    "X = mnist['data'].astype(np.float32) / 255.0\n",
    "y = mnist['target'].astype(\"int32\")\n",
    "X_train, X_valid, y_train, y_valid = sklearn.cross_validation.train_test_split(\n",
    "    X, y, random_state=42, train_size=50000, test_size=10000)\n",
    "X_train = X_train.reshape(-1, 1, 28, 28)\n",
    "X_valid = X_valid.reshape(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot a bunch of examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ############################## prepare model ##############################\n",
    "# architecture:\n",
    "# - 5x5 conv, 32 filters\n",
    "# - ReLU\n",
    "# - 2x2 maxpool\n",
    "# - 5x5 conv, 32 filters\n",
    "# - ReLU\n",
    "# - 2x2 maxpool\n",
    "# - fully connected layer - 256 units\n",
    "# - ReLU\n",
    "# - 50% dropout\n",
    "# - fully connected layer- 10 units\n",
    "# - softmax\n",
    "\n",
    "# - conv layers take in 4-tensors with the following dimensions:\n",
    "#   (batch size, number of channels, image dim 1, image dim 2)\n",
    "# - the batch size can be provided as `None` to make the network\n",
    "#   work for multiple different batch sizes\n",
    "l_in = lasagne.layers.InputLayer(\n",
    "    shape=(None, 1, 28, 28),\n",
    ")\n",
    "\n",
    "# - GlorotUniform is an intelligent initialization for conv layers\n",
    "#   that people like to use (: named after Xavier Glorot\n",
    "# - by default, a \"valid\" convolution\n",
    "# - note that ReLUs are specified in the nonlinearity\n",
    "l_conv1 = lasagne.layers.Conv2DLayer(\n",
    "    l_in,\n",
    "    num_filters=32,\n",
    "    filter_size=(5, 5),\n",
    "    nonlinearity=lasagne.nonlinearities.rectify,\n",
    "    W=lasagne.init.GlorotUniform(),\n",
    ")\n",
    "# - by default, the stride of the max pool is the same as it's\n",
    "#   receptive area\n",
    "l_pool1 = lasagne.layers.MaxPool2DLayer(l_conv1, pool_size=(2, 2))\n",
    "\n",
    "l_conv2 = lasagne.layers.Conv2DLayer(\n",
    "    l_pool1,\n",
    "    num_filters=32,\n",
    "    filter_size=(5, 5),\n",
    "    nonlinearity=lasagne.nonlinearities.rectify,\n",
    "    W=lasagne.init.GlorotUniform(),\n",
    ")\n",
    "l_pool2 = lasagne.layers.MaxPool2DLayer(l_conv2, pool_size=(2, 2))\n",
    "\n",
    "l_hidden1 = lasagne.layers.DenseLayer(\n",
    "    l_pool2,\n",
    "    num_units=256,\n",
    "    nonlinearity=lasagne.nonlinearities.rectify,\n",
    "    W=lasagne.init.GlorotUniform(),\n",
    ")\n",
    "\n",
    "# - applies the softmax after computing the final layer units\n",
    "# - note that there is no ReLU\n",
    "l_out = lasagne.layers.DenseLayer(\n",
    "    l_hidden1,\n",
    "    num_units=10,\n",
    "    nonlinearity=lasagne.nonlinearities.softmax,\n",
    "    W=lasagne.init.GlorotUniform(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ############################### network loss ###############################\n",
    "# int32 vector\n",
    "target_vector = T.ivector('y')\n",
    "\n",
    "\n",
    "def loss_fn(output):\n",
    "    return T.mean(lasagne.objectives.categorical_crossentropy(output,\n",
    "                                                              target_vector))\n",
    "\n",
    "output = lasagne.layers.get_output(l_out)\n",
    "loss = loss_fn(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling theano functions\n"
     ]
    }
   ],
   "source": [
    "# ######################## compiling theano functions ########################\n",
    "\n",
    "print(\"Compiling theano functions\")\n",
    "\n",
    "# - takes out all weight tensors from the network, in order to compute\n",
    "#   how the weights should be updated\n",
    "all_params = lasagne.layers.get_all_params(l_out)\n",
    "\n",
    "# - calculate how the parameters should be updated\n",
    "# - theano keeps a graph of operations, so that gradients w.r.t.\n",
    "#   the loss can be calculated\n",
    "updates = lasagne.updates.sgd(\n",
    "    loss_or_grads=loss,\n",
    "    params=all_params,\n",
    "    learning_rate=0.001)\n",
    "\n",
    "# - create a function that also updates the weights\n",
    "# - this function takes in 2 arguments: the input batch of images and a\n",
    "#   target vector (the y's) and returns a list with a single scalar\n",
    "#   element (the loss)\n",
    "train_fn = theano.function(inputs=[l_in.input_var, target_vector],\n",
    "                           outputs=[loss],\n",
    "                           updates=updates)\n",
    "\n",
    "# - same interface as previous the previous function, but now the\n",
    "#   output is a list where the first element is the loss, and the\n",
    "#   second element is the actual predicted probabilities for the\n",
    "#   input data\n",
    "valid_fn = theano.function(inputs=[l_in.input_var, target_vector],\n",
    "                           outputs=[loss, output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch: 1, train_loss=2.308017, valid_loss=2.300523, valid_accuracy=0.132400, time=68.693907s\n",
      "Epoch: 2, train_loss=2.291289, valid_loss=2.284946, valid_accuracy=0.161500, time=67.171433s\n",
      "Epoch: 3, train_loss=2.276676, valid_loss=2.270492, valid_accuracy=0.176400, time=64.029539s\n",
      "Epoch: 4, train_loss=2.262301, valid_loss=2.255666, valid_accuracy=0.185800, time=63.871153s\n",
      "Epoch: 5, train_loss=2.246940, valid_loss=2.239383, valid_accuracy=0.196100, time=67.484218s\n",
      "Epoch: 6, train_loss=2.229594, valid_loss=2.220664, valid_accuracy=0.219200, time=70.405469s\n",
      "Epoch: 7, train_loss=2.209297, valid_loss=2.198494, valid_accuracy=0.266500, time=64.971325s\n",
      "Epoch: 8, train_loss=2.184988, valid_loss=2.171744, valid_accuracy=0.330200, time=67.851746s\n",
      "Epoch: 9, train_loss=2.155298, valid_loss=2.138783, valid_accuracy=0.416400, time=61.432628s\n",
      "Epoch: 10, train_loss=2.118418, valid_loss=2.097501, valid_accuracy=0.503700, time=61.723150s\n",
      "Epoch: 11, train_loss=2.072133, valid_loss=2.045494, valid_accuracy=0.574200, time=61.753873s\n",
      "Epoch: 12, train_loss=2.013724, valid_loss=1.979713, valid_accuracy=0.630900, time=61.355060s\n",
      "Epoch: 13, train_loss=1.939999, valid_loss=1.896868, valid_accuracy=0.668600, time=61.266725s\n",
      "Epoch: 14, train_loss=1.847815, valid_loss=1.794404, valid_accuracy=0.696100, time=61.456767s\n",
      "Epoch: 15, train_loss=1.735477, valid_loss=1.671498, valid_accuracy=0.716400, time=61.534832s\n",
      "Epoch: 16, train_loss=1.604039, valid_loss=1.531299, valid_accuracy=0.734000, time=61.163098s\n",
      "Epoch: 17, train_loss=1.459242, valid_loss=1.382159, valid_accuracy=0.748700, time=61.146987s\n",
      "Epoch: 18, train_loss=1.311179, valid_loss=1.235595, valid_accuracy=0.763100, time=61.175322s\n",
      "Epoch: 19, train_loss=1.171093, valid_loss=1.101793, valid_accuracy=0.778600, time=60.974591s\n",
      "Epoch: 20, train_loss=1.047320, valid_loss=0.986805, valid_accuracy=0.790200, time=60.985364s\n",
      "Epoch: 21, train_loss=0.943366, valid_loss=0.891779, valid_accuracy=0.800400, time=61.120126s\n",
      "Epoch: 22, train_loss=0.858455, valid_loss=0.814472, valid_accuracy=0.809100, time=60.919617s\n",
      "Epoch: 23, train_loss=0.789576, valid_loss=0.751550, valid_accuracy=0.817700, time=60.788476s\n",
      "Epoch: 24, train_loss=0.733302, valid_loss=0.699845, valid_accuracy=0.826000, time=61.149913s\n",
      "Epoch: 25, train_loss=0.686743, valid_loss=0.656755, valid_accuracy=0.833400, time=60.901934s\n"
     ]
    }
   ],
   "source": [
    "# ################################# training #################################\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "num_epochs = 25\n",
    "batch_size = 600\n",
    "for epoch_num in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    # iterate over training minibatches and update the weights\n",
    "    num_batches_train = int(np.ceil(len(X_train) / batch_size))\n",
    "    train_losses = []\n",
    "    for batch_num in range(num_batches_train):\n",
    "        batch_slice = slice(batch_size * batch_num,\n",
    "                            batch_size * (batch_num + 1))\n",
    "        X_batch = X_train[batch_slice]\n",
    "        y_batch = y_train[batch_slice]\n",
    "\n",
    "        loss, = train_fn(X_batch, y_batch)\n",
    "        train_losses.append(loss)\n",
    "    # aggregate training losses for each minibatch into scalar\n",
    "    train_loss = np.mean(train_losses)\n",
    "\n",
    "    # calculate validation loss\n",
    "    num_batches_valid = int(np.ceil(len(X_valid) / batch_size))\n",
    "    valid_losses = []\n",
    "    list_of_probabilities_batch = []\n",
    "    for batch_num in range(num_batches_valid):\n",
    "        batch_slice = slice(batch_size * batch_num,\n",
    "                            batch_size * (batch_num + 1))\n",
    "        X_batch = X_valid[batch_slice]\n",
    "        y_batch = y_valid[batch_slice]\n",
    "\n",
    "        loss, probabilities_batch = valid_fn(X_batch, y_batch)\n",
    "        valid_losses.append(loss)\n",
    "        list_of_probabilities_batch.append(probabilities_batch)\n",
    "    valid_loss = np.mean(valid_losses)\n",
    "    # concatenate probabilities for each batch into a matrix\n",
    "    probabilities = np.concatenate(list_of_probabilities_batch)\n",
    "    # calculate classes from the probabilities\n",
    "    predicted_classes = np.argmax(probabilities, axis=1)\n",
    "    # calculate accuracy for this epoch\n",
    "    accuracy = sklearn.metrics.accuracy_score(y_valid, predicted_classes)\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(\"Epoch: %d, train_loss=%f, valid_loss=%f, valid_accuracy=%f, time=%fs\"\n",
    "          % (epoch_num + 1, train_loss, valid_loss, accuracy, total_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
